{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Initial Setup and GPU Check"
      ],
      "metadata": {
        "id": "c4kQpDgeXzJP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSUHtaDaXww_"
      },
      "outputs": [],
      "source": [
        "#@title Initial Setup and GPU Check\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Check GPU availability and type\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"✓ GPU Available: {gpu_name}\")\n",
        "    print(f\"✓ CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"✓ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Verify we have A100\n",
        "    if \"A100\" not in gpu_name:\n",
        "        print(\"⚠️  Warning: Not running on A100. Performance may vary.\")\n",
        "else:\n",
        "    raise RuntimeError(\"❌ No GPU available! Please enable GPU in Runtime > Change runtime type\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(\"\\n✓ Initial setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Install Dependencies"
      ],
      "metadata": {
        "id": "FuxS4C7RX0Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install DeepSpeed from GitHub\n",
        "%%time\n",
        "\n",
        "# First ensure build tools\n",
        "!pip install ninja packaging\n",
        "\n",
        "# Clone and install deepspeed from source\n",
        "!git clone https://github.com/microsoft/DeepSpeed.git\n",
        "%cd DeepSpeed\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "\n",
        "# Install other deps\n",
        "!pip install -U pytorch-lightning transformers datasets safetensors einops wandb huggingface-hub torchmetrics msgpack\n",
        "\n",
        "print(\"✓ DeepSpeed built from source\")"
      ],
      "metadata": {
        "id": "z6kTtvSBXx4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Clone and Setup RWKV-LM Repository"
      ],
      "metadata": {
        "id": "IAEmrYVeX-98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone RWKV-LM and Setup Environment\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Clone RWKV-LM repository\n",
        "if os.path.exists('RWKV-LM'):\n",
        "    shutil.rmtree('RWKV-LM')\n",
        "\n",
        "!git clone https://github.com/BlinkDL/RWKV-LM.git\n",
        "%cd RWKV-LM\n",
        "\n",
        "# Set environment variables for RWKV\n",
        "os.environ[\"RWKV_JIT_ON\"] = \"1\"\n",
        "os.environ[\"RWKV_CUDA_ON\"] = \"1\"\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"gazelle_implementation\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "print(\"✓ RWKV-LM repository cloned and environment configured!\")"
      ],
      "metadata": {
        "id": "9AAlzNmbYAjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Mount Google Drive for Persistent Storage"
      ],
      "metadata": {
        "id": "KbtoyUXdYDMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive for Checkpoints\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Create checkpoint directory in Drive\n",
        "checkpoint_path = '/content/drive/MyDrive/gazelle_0.5b_checkpoints'\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "os.makedirs(f\"{checkpoint_path}/models\", exist_ok=True)\n",
        "os.makedirs(f\"{checkpoint_path}/logs\", exist_ok=True)\n",
        "os.makedirs(f\"{checkpoint_path}/configs\", exist_ok=True)\n",
        "\n",
        "print(f\"✓ Google Drive mounted!\")\n",
        "print(f\"✓ Checkpoint directory: {checkpoint_path}\")\n",
        "\n",
        "# Create symlink for easy access\n",
        "if os.path.exists('/content/gazelle_checkpoints'):\n",
        "    os.unlink('/content/gazelle_checkpoints')\n",
        "os.symlink(checkpoint_path, '/content/gazelle_checkpoints')"
      ],
      "metadata": {
        "id": "oRsgeGDIYE-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: Download and Prepare Dolphin Distill Dataset"
      ],
      "metadata": {
        "id": "MW7aJBTTYIa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and Prepare Dolphin Distill Dataset (FIXED)\n",
        "%%time\n",
        "\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "%cd data\n",
        "\n",
        "print(\"Loading Dolphin Distill dataset from HuggingFace...\")\n",
        "dataset = load_dataset(\"cognitivecomputations/dolphin-distill\", split=\"train\")\n",
        "print(f\"✓ Dataset loaded! Total examples: {len(dataset):,}\")\n",
        "\n",
        "# First, let's SEE what's happening with wget\n",
        "print(\"\\nDownloading RWKV 20B tokenizer...\")\n",
        "print(\"Checking available tokenizer files...\")\n",
        "\n",
        "# Remove -q to see actual error\n",
        "!wget https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-v5/20B_tokenizer.json\n",
        "\n",
        "# If that fails, try alternative URLs\n",
        "if not os.path.exists(\"20B_tokenizer.json\"):\n",
        "    print(\"\\nTrying alternative URL...\")\n",
        "    # Try the v4 tokenizer which is more stable\n",
        "    !wget https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/tokenizer/rwkv_vocab_v20230424.json -O 20B_tokenizer.json\n",
        "\n",
        "# If still failing, try the model repo\n",
        "if not os.path.exists(\"20B_tokenizer.json\"):\n",
        "    print(\"\\nTrying HuggingFace model repo...\")\n",
        "    !wget https://huggingface.co/BlinkDL/rwkv-4-world/resolve/main/tokenizer.json -O 20B_tokenizer.json\n",
        "\n",
        "# Check what we got\n",
        "!ls -la *.json\n",
        "\n",
        "# Continue with rest of code..."
      ],
      "metadata": {
        "id": "KUeAdyJEYI_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset and create directories\n",
        "import os\n",
        "os.chdir('/content')\n",
        "os.makedirs('RWKV-LM/gazelle_implementation', exist_ok=True)\n",
        "os.chdir('/content/RWKV-LM')\n",
        "print(f\"Current directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "sXpSv2x2mB16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Core Gazelle Architecture Implementation"
      ],
      "metadata": {
        "id": "XLxrPE4oYOnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gazelle 0.5B Core Architecture\n",
        "\n",
        "# First, reset to home directory to avoid nested path issues\n",
        "%cd /content\n",
        "\n",
        "# Check where we are\n",
        "!pwd\n",
        "\n",
        "# Create the directory structure if it doesn't exist\n",
        "!mkdir -p /content/RWKV-LM/gazelle_implementation\n",
        "\n",
        "# Now cd to the correct directory\n",
        "%cd /content/RWKV-LM\n",
        "\n",
        "# Verify we're in the right place\n",
        "!pwd"
      ],
      "metadata": {
        "id": "tmvTus1cnL7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gazelle_implementation/gazelle_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GazelleConfig:\n",
        "    n_layer: int = 12\n",
        "    n_embd: int = 1536\n",
        "    n_head: int = 24\n",
        "    vocab_size: int = 65536\n",
        "    ctx_len: int = 512\n",
        "       # GhostRNN parameters\n",
        "    ghost_ratio: float = 0.375\n",
        "    use_ghost: bool = True\n",
        "\n",
        "    # Thinking parameters\n",
        "    enable_thinking: bool = False\n",
        "    max_think_steps: int = 5\n",
        "    think_threshold: float = 0.3\n",
        "\n",
        "    # RWKV-7 specific\n",
        "    head_qk: int = 0\n",
        "    time_decay_init: str = \"log\"\n",
        "\n",
        "    @property\n",
        "    def head_dim(self):\n",
        "        return self.n_embd // self.n_head\n",
        "\n",
        "    @property\n",
        "    def intrinsic_dim(self):\n",
        "        return int(self.head_dim * (1 - self.ghost_ratio))\n",
        "\n",
        "    @property\n",
        "    def ghost_dim(self):\n",
        "        return self.head_dim - self.intrinsic_dim\n",
        "\n",
        "class GhostRWKV7State(nn.Module):\n",
        "    \"\"\"Implements GhostRNN decomposition for RWKV-7 matrix states\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Ghost transformation layers (cheap operations)\n",
        "        self.ghost_linear = nn.ModuleList([\n",
        "            nn.Linear(config.intrinsic_dim, config.ghost_dim, bias=False)\n",
        "            for _ in range(config.n_head)\n",
        "        ])\n",
        "\n",
        "        # Initialize ghost transforms near identity\n",
        "        for linear in self.ghost_linear:\n",
        "            nn.init.xavier_uniform_(linear.weight, gain=0.1)\n",
        "\n",
        "    def split_state(self, state):\n",
        "        \"\"\"Split state into intrinsic and ghost parts\"\"\"\n",
        "        B, H, D1, D2 = state.shape\n",
        "        intrinsic = state[:, :, :self.config.intrinsic_dim, :self.config.intrinsic_dim]\n",
        "        return intrinsic\n",
        "\n",
        "    def generate_ghost(self, intrinsic, head_idx):\n",
        "        \"\"\"Generate ghost state from intrinsic state\"\"\"\n",
        "        B, D1, D2 = intrinsic.shape\n",
        "\n",
        "        # Apply cheap linear transform\n",
        "        ghost_rows = self.ghost_linear[head_idx](intrinsic.transpose(-1, -2))\n",
        "        ghost_cols = self.ghost_linear[head_idx](intrinsic)\n",
        "\n",
        "        # Combine to form ghost block\n",
        "        ghost = torch.zeros(B, self.config.ghost_dim, self.config.ghost_dim,\n",
        "                          device=intrinsic.device, dtype=intrinsic.dtype)\n",
        "\n",
        "        # Fill ghost state (simplified for initial implementation)\n",
        "        ghost = ghost_rows.transpose(-1, -2)[:, :self.config.ghost_dim, :]\n",
        "\n",
        "        return ghost\n",
        "\n",
        "    def combine_state(self, intrinsic, ghost):\n",
        "        \"\"\"Recombine intrinsic and ghost into full state\"\"\"\n",
        "        B, H = intrinsic.shape[:2]\n",
        "        D = self.config.head_dim\n",
        "\n",
        "        # Create full state matrix\n",
        "        full_state = torch.zeros(B, H, D, D, device=intrinsic.device, dtype=intrinsic.dtype)\n",
        "\n",
        "        # Fill intrinsic part\n",
        "        i_dim = self.config.intrinsic_dim\n",
        "        full_state[:, :, :i_dim, :i_dim] = intrinsic\n",
        "\n",
        "        # Fill ghost part\n",
        "        full_state[:, :, i_dim:, i_dim:] = ghost\n",
        "\n",
        "        # Cross terms (can be refined later)\n",
        "        full_state[:, :, :i_dim, i_dim:] = intrinsic.mean(dim=-1, keepdim=True).expand(-1, -1, -1, self.config.ghost_dim) * 0.1\n",
        "        full_state[:, :, i_dim:, :i_dim] = ghost.mean(dim=-1, keepdim=True).expand(-1, -1, -1, i_dim) * 0.1\n",
        "\n",
        "        return full_state\n",
        "\n",
        "class ThinkingModule(nn.Module):\n",
        "    \"\"\"Adaptive thinking mechanism for Gazelle\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Complexity estimator network\n",
        "        self.complexity_net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Thinking refinement parameters\n",
        "        self.think_k = nn.Parameter(torch.zeros(1, config.n_head, 1, config.head_dim))\n",
        "        self.think_v = nn.Parameter(torch.zeros(1, config.n_head, 1, config.head_dim))\n",
        "        self.think_decay = nn.Parameter(torch.ones(1, config.n_head, config.head_dim))\n",
        "\n",
        "        # Initialize thinking parameters\n",
        "        nn.init.normal_(self.think_k, std=0.02)\n",
        "        nn.init.normal_(self.think_v, std=0.02)\n",
        "\n",
        "    def estimate_complexity(self, x, state):\n",
        "        \"\"\"Estimate complexity of current position\"\"\"\n",
        "        # Combine input and state info\n",
        "        state_summary = state.mean(dim=(2, 3)).flatten(1)  # [B, H*D]\n",
        "        combined = torch.cat([x, state_summary], dim=-1)\n",
        "\n",
        "        # Estimate complexity\n",
        "        complexity = self.complexity_net(x).squeeze(-1)  # [B]\n",
        "\n",
        "        return complexity\n",
        "\n",
        "    def compute_think_steps(self, complexity, training=False):\n",
        "        \"\"\"Determine number of thinking steps needed\"\"\"\n",
        "        if not self.config.enable_thinking:\n",
        "            return 1\n",
        "\n",
        "        # During training, sometimes force different step counts\n",
        "        if training:\n",
        "            # Curriculum: gradually increase max steps\n",
        "            # Need a way to track training progress, maybe pass it in\n",
        "            # For now, use a placeholder or simpler curriculum\n",
        "            max_steps = self.config.max_think_steps # Simplified for now\n",
        "        else:\n",
        "            max_steps = self.config.max_think_steps\n",
        "\n",
        "        # Threshold-based step calculation\n",
        "        steps = torch.where(\n",
        "            complexity > self.config.think_threshold,\n",
        "            torch.clamp((complexity * max_steps).int() + 1, 1, max_steps),\n",
        "            torch.ones_like(complexity, dtype=torch.int)\n",
        "        )\n",
        "\n",
        "        return steps\n",
        "\n",
        "    def thinking_step(self, state, wkv_func):\n",
        "        \"\"\"Execute one thinking refinement step\"\"\"\n",
        "        B, H, D1, D2 = state.shape\n",
        "\n",
        "        # Create think keys and values\n",
        "        k = self.think_k.expand(B, -1, D1, -1)\n",
        "        v = self.think_v.expand(B, -1, D1, -1)\n",
        "\n",
        "        # Apply thinking WKV operation\n",
        "        # Need a WKV implementation that works with state\n",
        "        # This is a placeholder, RWKV-7 WKV is complex\n",
        "        # For now, a simplified state update\n",
        "        refined_state = state * torch.exp(-self.think_decay.unsqueeze(-1)) + k.transpose(-1,-2) @ v\n",
        "\n",
        "        return refined_state\n",
        "\n",
        "class GazelleRWKV7Layer(nn.Module):\n",
        "    \"\"\"Single Gazelle layer combining RWKV-7, GhostRNN, and Thinking\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Layer norms\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        # RWKV-7 Time Mixing\n",
        "        self.time_mixing = RWKV7TimeMixing(config, layer_idx)\n",
        "\n",
        "        # RWKV-7 Channel Mixing\n",
        "        self.channel_mixing = RWKV7ChannelMixing(config, layer_idx)\n",
        "\n",
        "        # Ghost state handler\n",
        "        if config.use_ghost:\n",
        "            self.ghost_state = GhostRWKV7State(config)\n",
        "\n",
        "        # Thinking module (only in middle layers)\n",
        "        if config.enable_thinking and 3 <= layer_idx <= config.n_layer - 3:\n",
        "            self.thinking = ThinkingModule(config)\n",
        "        else:\n",
        "            self.thinking = None\n",
        "\n",
        "    def forward(self, x, state=None, use_thinking=True):\n",
        "        # Store thinking info if enabled\n",
        "        thinking_info = {}\n",
        "\n",
        "        # Time mixing with optional ghost states\n",
        "        current_state = state\n",
        "        if self.config.use_ghost and current_state is not None:\n",
        "            # Decompose state\n",
        "            intrinsic = self.ghost_state.split_state(current_state)\n",
        "            ghost = torch.stack([\n",
        "                self.ghost_state.generate_ghost(intrinsic[:, h], h)\n",
        "                for h in range(self.config.n_head)\n",
        "            ], dim=1)\n",
        "            current_state = self.ghost_state.combine_state(intrinsic, ghost)\n",
        "\n",
        "        # Thinking steps if enabled\n",
        "        if self.thinking is not None and use_thinking:\n",
        "            complexity = self.thinking.estimate_complexity(x, current_state)\n",
        "            think_steps = self.thinking.compute_think_steps(complexity, self.training)\n",
        "\n",
        "            thinking_info['complexities'] = complexity.mean().item()\n",
        "            thinking_info['steps'] = think_steps.float().mean().item()\n",
        "            thinking_states_list = [current_state.clone()] # Store initial state\n",
        "\n",
        "            # Execute thinking steps\n",
        "            for step in range(think_steps.max().item()):\n",
        "                # Only apply thinking step for batches that require this step\n",
        "                mask = think_steps > step\n",
        "                if mask.any():\n",
        "                    current_state[mask] = self.thinking.thinking_step(current_state[mask], self.time_mixing.wkv)\n",
        "                    thinking_states_list.append(current_state.clone())\n",
        "\n",
        "            thinking_info['states'] = thinking_states_list\n",
        "\n",
        "        # Standard RWKV-7 forward pass\n",
        "        # The WKV function needs to be able to handle the state argument\n",
        "        x = x + self.time_mixing(self.ln1(x), current_state)\n",
        "        x = x + self.channel_mixing(self.ln2(x))\n",
        "\n",
        "        return x, current_state, thinking_info\n",
        "\n",
        "class RWKV7TimeMixing(nn.Module):\n",
        "    \"\"\"RWKV-7 Time Mixing implementation\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Receptance, Key, Value, Gate projections\n",
        "        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.gate = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.output = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "\n",
        "        # Time decay and bonus parameters\n",
        "        # Shape should be [H, D]\n",
        "        self.time_decay = nn.Parameter(torch.zeros(config.n_head, config.head_dim))\n",
        "        self.time_bonus = nn.Parameter(torch.zeros(config.n_head, config.head_dim))\n",
        "\n",
        "        # Layer-specific initialization\n",
        "        ratio_0_to_1 = layer_idx / max(config.n_layer - 1, 1)\n",
        "        ratio_1_to_almost0 = 1.0 - (layer_idx / config.n_layer)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.orthogonal_(self.receptance.weight, gain=1)\n",
        "        nn.init.orthogonal_(self.key.weight, gain=0.1)\n",
        "        nn.init.orthogonal_(self.value.weight, gain=1)\n",
        "        nn.init.orthogonal_(self.gate.weight, gain=0.1)\n",
        "        nn.init.zeros_(self.output.weight)\n",
        "\n",
        "        # Initialize time decay\n",
        "        with torch.no_grad():\n",
        "            # RWKV-7 decay initialization\n",
        "            decay_values = -5 + 8 * ratio_0_to_1\n",
        "            self.time_decay.uniform_(decay_values - 0.5, decay_values + 0.5)\n",
        "\n",
        "    def wkv(self, state, k, v, decay):\n",
        "        \"\"\"WKV computation for RWKV-7\"\"\"\n",
        "        # This needs to be the actual efficient CUDA/optimized implementation\n",
        "        # The current implementation is a simplified placeholder and will be slow\n",
        "        # For training, you'll need the official RWKV CUDA kernel\n",
        "        # https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/cuda/wkv_cuda.py\n",
        "\n",
        "        # Simplified placeholder WKV (DO NOT USE FOR REAL TRAINING)\n",
        "        B, H, T, D = k.shape\n",
        "\n",
        "        # state shape should be [B, H, D, D] for full matrix state in RWKV-7\n",
        "        # Or [B, H, 2, D] for simplified state\n",
        "        # The current state shape in GazelleModel is None or handled by GhostState.\n",
        "        # Need to align state representation. Assuming state is None or [B, H, D, D] for now.\n",
        "\n",
        "        output = torch.zeros_like(v)\n",
        "        current_state = state if state is not None else torch.zeros(B, H, D, D, device=k.device, dtype=k.dtype)\n",
        "\n",
        "        for t in range(T):\n",
        "            # This is NOT the correct RWKV-7 state update\n",
        "            # This is a generic RNN-like update for the placeholder\n",
        "            current_state = current_state * decay.unsqueeze(-1) + k[:, :, t:t+1].transpose(-1, -2) @ v[:, :, t:t+1]\n",
        "            output[:, :, t] = (current_state @ k[:, :, t:t+1].transpose(-1, -2)).squeeze(-1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, state=None):\n",
        "        B, T, C = x.shape\n",
        "        H = self.config.n_head\n",
        "        D = self.config.head_dim\n",
        "\n",
        "        # Compute RKWG\n",
        "        r = self.receptance(x).view(B, T, H, D).transpose(1, 2)\n",
        "        k = self.key(x).view(B, T, H, D).transpose(1, 2)\n",
        "        v = self.value(x).view(B, T, H, D).transpose(1, 2)\n",
        "        g = torch.sigmoid(self.gate(x)).view(B, T, H, D).transpose(1, 2)\n",
        "\n",
        "        # Apply WKV\n",
        "        # Need to pass state to WKV\n",
        "        x = self.wkv(state, k, v, torch.exp(-torch.exp(self.time_decay)))\n",
        "\n",
        "        # Apply receptance and gate\n",
        "        x = x * torch.sigmoid(r) * g\n",
        "\n",
        "        # Combine heads and output\n",
        "        x = x.transpose(1, 2).reshape(B, T, C)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class RWKV7ChannelMixing(nn.Module):\n",
        "    \"\"\"RWKV-7 Channel Mixing (FFN) implementation\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # FFN components\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd * 4, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd * 4, config.n_embd, bias=False)\n",
        "        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "\n",
        "        # Initialize\n",
        "        nn.init.orthogonal_(self.key.weight, gain=1)\n",
        "        nn.init.zeros_(self.value.weight)\n",
        "        nn.init.zeros_(self.receptance.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # RWKV-7 style FFN with receptance gate\n",
        "        k = self.key(x)\n",
        "        k = torch.relu(k) ** 2  # Square ReLU activation\n",
        "        v = self.value(k)\n",
        "        r = torch.sigmoid(self.receptance(x))\n",
        "\n",
        "        return r * v\n",
        "\n",
        "class GazelleModel(nn.Module):\n",
        "    \"\"\"Complete Gazelle 0.5B Model\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Token embeddings\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.ln_emb = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        # Gazelle layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            GazelleRWKV7Layer(config, idx) for idx in range(config.n_layer)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.ln_head = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize embeddings\n",
        "        nn.init.uniform_(self.embedding.weight, -1e-4, 1e-4)\n",
        "        nn.init.orthogonal_(self.head.weight, gain=0.5 * math.sqrt(config.vocab_size / config.n_embd))\n",
        "\n",
        "        # State management - list of states, one per layer\n",
        "        self.states = [None] * self.config.n_layer\n",
        "\n",
        "        # Thinking info storage\n",
        "        self.thinking_info = {}\n",
        "\n",
        "        print(f\"✓ Gazelle Model initialized with {self.num_parameters():.2f}M parameters\")\n",
        "        if config.use_ghost:\n",
        "            print(f\"  - Ghost ratio: {config.ghost_ratio:.1%} reduction\")\n",
        "        if config.enable_thinking:\n",
        "            print(f\"  - Thinking enabled: max {config.max_think_steps} steps\")\n",
        "\n",
        "    def num_parameters(self):\n",
        "        \"\"\"Count trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad) / 1e6\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Token embeddings\n",
        "        x = self.embedding(idx)\n",
        "        x = self.ln_emb(x)\n",
        "\n",
        "        # Clear thinking info\n",
        "        self.thinking_info = {'complexities': [], 'steps': [], 'state_changes': []}\n",
        "\n",
        "        # Forward through layers\n",
        "        new_states = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, new_state, layer_thinking_info = layer(x, self.states[i])\n",
        "            new_states.append(new_state)\n",
        "\n",
        "            # Accumulate thinking info\n",
        "            if self.config.enable_thinking and layer.thinking is not None:\n",
        "                self.thinking_info['complexities'].append(layer_thinking_info.get('complexities', 0.0))\n",
        "                self.thinking_info['steps'].append(layer_thinking_info.get('steps', 0.0))\n",
        "                # State change info is calculated in ThinkingLossCalculator\n",
        "\n",
        "        self.states = new_states # Update states\n",
        "\n",
        "        # Output projection\n",
        "        x = self.ln_head(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        # Calculate loss if targets provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Reshape for cross_entropy: [N, C] and [N]\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_tokens=100, temperature=1.0, top_p=0.9):\n",
        "        \"\"\"Generate text with thinking steps tracking\"\"\"\n",
        "        self.eval()\n",
        "        generated = []\n",
        "        thinking_log = []\n",
        "        current_idx = idx.clone()\n",
        "\n",
        "        # Initialize states for generation\n",
        "        self.states = [None] * self.config.n_layer\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_tokens):\n",
        "                # Forward pass (process only the last token)\n",
        "                logits, _ = self(current_idx[:, -1].unsqueeze(0)) # Process one token at a time\n",
        "\n",
        "                logits = logits[:, -1, :] / temperature\n",
        "\n",
        "                # Top-p sampling\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "                cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
        "                mask = cumsum > top_p\n",
        "                mask[:, 0] = False  # Keep at least one token\n",
        "                sorted_probs[mask] = 0\n",
        "                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "                # Sample\n",
        "                next_token = torch.multinomial(sorted_probs, 1)\n",
        "                next_token = sorted_indices.gather(-1, next_token)\n",
        "\n",
        "                # Track thinking if enabled (aggregate across layers)\n",
        "                if self.config.enable_thinking and self.thinking_info:\n",
        "                    avg_complexity = sum(self.thinking_info.get('complexities', [])) / max(1, len(self.thinking_info.get('complexities', [])))\n",
        "                    avg_steps = sum(self.thinking_info.get('steps', [])) / max(1, len(self.thinking_info.get('steps', [])))\n",
        "\n",
        "                    thinking_log.append({\n",
        "                        'token_id': next_token.item(),\n",
        "                        'avg_complexity': avg_complexity,\n",
        "                        'avg_think_steps': avg_steps\n",
        "                    })\n",
        "                    # Clear thinking info after processing one token\n",
        "                    self.thinking_info = {'complexities': [], 'steps': [], 'state_changes': []}\n",
        "\n",
        "\n",
        "                # Append to sequence\n",
        "                current_idx = torch.cat([current_idx, next_token], dim=1)\n",
        "                generated.append(next_token.item())\n",
        "\n",
        "                # Stop on EOS token (assuming 0 is EOS, adjust if needed)\n",
        "                if next_token.item() == 0:\n",
        "                    break\n",
        "\n",
        "        return generated, thinking_log\n",
        "\n",
        "# Save the configuration\n",
        "config = GazelleConfig()"
      ],
      "metadata": {
        "id": "MIJhEuscnQxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gazelle 0.5B Core Architecture\n",
        "%cd /content/RWKV-LM # Ensure we are in the correct directory\n",
        "\n",
        "%%writefile gazelle_implementation/gazelle_model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GazelleConfig:\n",
        "    n_layer: int = 12\n",
        "    n_embd: int = 1536\n",
        "    n_head: int = 24\n",
        "    vocab_size: int = 65536\n",
        "    ctx_len: int = 512\n",
        "\n",
        "    # GhostRNN parameters\n",
        "    ghost_ratio: float = 0.375\n",
        "    use_ghost: bool = True\n",
        "\n",
        "    # Thinking parameters\n",
        "    enable_thinking: bool = False\n",
        "    max_think_steps: int = 5\n",
        "    think_threshold: float = 0.3\n",
        "\n",
        "    # RWKV-7 specific\n",
        "    head_qk: int = 0\n",
        "    time_decay_init: str = \"log\"\n",
        "\n",
        "    @property\n",
        "    def head_dim(self):\n",
        "        return self.n_embd // self.n_head\n",
        "\n",
        "    @property\n",
        "    def intrinsic_dim(self):\n",
        "        return int(self.head_dim * (1 - self.ghost_ratio))\n",
        "\n",
        "    @property\n",
        "    def ghost_dim(self):\n",
        "        return self.head_dim - self.intrinsic_dim\n",
        "\n",
        "class GhostRWKV7State(nn.Module):\n",
        "    \"\"\"Implements GhostRNN decomposition for RWKV-7 matrix states\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Ghost transformation layers (cheap operations)\n",
        "        self.ghost_linear = nn.ModuleList([\n",
        "            nn.Linear(config.intrinsic_dim, config.ghost_dim, bias=False)\n",
        "            for _ in range(config.n_head)\n",
        "        ])\n",
        "\n",
        "        # Initialize ghost transforms near identity\n",
        "        for linear in self.ghost_linear:\n",
        "            nn.init.xavier_uniform_(linear.weight, gain=0.1)\n",
        "\n",
        "    def split_state(self, state):\n",
        "        \"\"\"Split state into intrinsic and ghost parts\"\"\"\n",
        "        B, H, D1, D2 = state.shape\n",
        "        intrinsic = state[:, :, :self.config.intrinsic_dim, :self.config.intrinsic_dim]\n",
        "        return intrinsic\n",
        "\n",
        "    def generate_ghost(self, intrinsic, head_idx):\n",
        "        \"\"\"Generate ghost state from intrinsic state\"\"\"\n",
        "        B, D1, D2 = intrinsic.shape\n",
        "\n",
        "        # Apply cheap linear transform\n",
        "        ghost_rows = self.ghost_linear[head_idx](intrinsic.transpose(-1, -2))\n",
        "        ghost_cols = self.ghost_linear[head_idx](intrinsic)\n",
        "\n",
        "        # Combine to form ghost block\n",
        "        ghost = torch.zeros(B, self.config.ghost_dim, self.config.ghost_dim,\n",
        "                          device=intrinsic.device, dtype=intrinsic.dtype)\n",
        "\n",
        "        # Fill ghost state (simplified for initial implementation)\n",
        "        ghost = ghost_rows.transpose(-1, -2)[:, :self.config.ghost_dim, :]\n",
        "\n",
        "        return ghost\n",
        "\n",
        "    def combine_state(self, intrinsic, ghost):\n",
        "        \"\"\"Recombine intrinsic and ghost into full state\"\"\"\n",
        "        B, H = intrinsic.shape[:2]\n",
        "        D = self.config.head_dim\n",
        "\n",
        "        # Create full state matrix\n",
        "        full_state = torch.zeros(B, H, D, D, device=intrinsic.device, dtype=intrinsic.dtype)\n",
        "\n",
        "        # Fill intrinsic part\n",
        "        i_dim = self.config.intrinsic_dim\n",
        "        full_state[:, :, :i_dim, :i_dim] = intrinsic\n",
        "\n",
        "        # Fill ghost part\n",
        "        full_state[:, :, i_dim:, i_dim:] = ghost\n",
        "\n",
        "        # Cross terms (can be refined later)\n",
        "        full_state[:, :, :i_dim, i_dim:] = intrinsic.mean(dim=-1, keepdim=True).expand(-1, -1, -1, self.config.ghost_dim) * 0.1\n",
        "        full_state[:, :, i_dim:, :i_dim] = ghost.mean(dim=-1, keepdim=True).expand(-1, -1, -1, i_dim) * 0.1\n",
        "\n",
        "        return full_state\n",
        "\n",
        "class ThinkingModule(nn.Module):\n",
        "    \"\"\"Adaptive thinking mechanism for Gazelle\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Complexity estimator network\n",
        "        self.complexity_net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Thinking refinement parameters\n",
        "        self.think_k = nn.Parameter(torch.zeros(1, config.n_head, 1, config.head_dim))\n",
        "        self.think_v = nn.Parameter(torch.zeros(1, config.n_head, 1, config.head_dim))\n",
        "        self.think_decay = nn.Parameter(torch.ones(1, config.n_head, config.head_dim))\n",
        "\n",
        "        # Initialize thinking parameters\n",
        "        nn.init.normal_(self.think_k, std=0.02)\n",
        "        nn.init.normal_(self.think_v, std=0.02)\n",
        "\n",
        "    def estimate_complexity(self, x, state):\n",
        "        \"\"\"Estimate complexity of current position\"\"\"\n",
        "        # Combine input and state info\n",
        "        state_summary = state.mean(dim=(2, 3)).flatten(1)  # [B, H*D]\n",
        "        combined = torch.cat([x, state_summary], dim=-1)\n",
        "\n",
        "        # Estimate complexity\n",
        "        complexity = self.complexity_net(x).squeeze(-1)  # [B]\n",
        "\n",
        "        return complexity\n",
        "\n",
        "    def compute_think_steps(self, complexity, training=False):\n",
        "        \"\"\"Determine number of thinking steps needed\"\"\"\n",
        "        if not self.config.enable_thinking:\n",
        "            return 1\n",
        "\n",
        "        # During training, sometimes force different step counts\n",
        "        if training:\n",
        "            # Curriculum: gradually increase max steps\n",
        "            # Need a way to track training progress, maybe pass it in\n",
        "            # For now, use a placeholder or simpler curriculum\n",
        "            max_steps = self.config.max_think_steps # Simplified for now\n",
        "        else:\n",
        "            max_steps = self.config.max_think_steps\n",
        "\n",
        "        # Threshold-based step calculation\n",
        "        steps = torch.where(\n",
        "            complexity > self.config.think_threshold,\n",
        "            torch.clamp((complexity * max_steps).int() + 1, 1, max_steps),\n",
        "            torch.ones_like(complexity, dtype=torch.int)\n",
        "        )\n",
        "\n",
        "        return steps\n",
        "\n",
        "    def thinking_step(self, state, wkv_func):\n",
        "        \"\"\"Execute one thinking refinement step\"\"\"\n",
        "        B, H, D1, D2 = state.shape\n",
        "\n",
        "        # Create think keys and values\n",
        "        k = self.think_k.expand(B, -1, D1, -1)\n",
        "        v = self.think_v.expand(B, -1, D1, -1)\n",
        "\n",
        "        # Apply thinking WKV operation\n",
        "        # Need a WKV implementation that works with state\n",
        "        # This is a placeholder, RWKV-7 WKV is complex\n",
        "        # For now, a simplified state update\n",
        "        refined_state = state * torch.exp(-self.think_decay.unsqueeze(-1)) + k.transpose(-1,-2) @ v\n",
        "\n",
        "        return refined_state\n",
        "\n",
        "class GazelleRWKV7Layer(nn.Module):\n",
        "    \"\"\"Single Gazelle layer combining RWKV-7, GhostRNN, and Thinking\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Layer norms\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        # RWKV-7 Time Mixing\n",
        "        self.time_mixing = RWKV7TimeMixing(config, layer_idx)\n",
        "\n",
        "        # RWKV-7 Channel Mixing\n",
        "        self.channel_mixing = RWKV7ChannelMixing(config, layer_idx)\n",
        "\n",
        "        # Ghost state handler\n",
        "        if config.use_ghost:\n",
        "            self.ghost_state = GhostRWKV7State(config)\n",
        "\n",
        "        # Thinking module (only in middle layers)\n",
        "        if config.enable_thinking and 3 <= layer_idx <= config.n_layer - 3:\n",
        "            self.thinking = ThinkingModule(config)\n",
        "        else:\n",
        "            self.thinking = None\n",
        "\n",
        "    def forward(self, x, state=None, use_thinking=True):\n",
        "        # Store thinking info if enabled\n",
        "        thinking_info = {}\n",
        "\n",
        "        # Time mixing with optional ghost states\n",
        "        current_state = state\n",
        "        if self.config.use_ghost and current_state is not None:\n",
        "            # Decompose state\n",
        "            intrinsic = self.ghost_state.split_state(current_state)\n",
        "            ghost = torch.stack([\n",
        "                self.ghost_state.generate_ghost(intrinsic[:, h], h)\n",
        "                for h in range(self.config.n_head)\n",
        "            ], dim=1)\n",
        "            current_state = self.ghost_state.combine_state(intrinsic, ghost)\n",
        "\n",
        "        # Thinking steps if enabled\n",
        "        if self.thinking is not None and use_thinking:\n",
        "            complexity = self.thinking.estimate_complexity(x, current_state)\n",
        "            think_steps = self.thinking.compute_think_steps(complexity, self.training)\n",
        "\n",
        "            thinking_info['complexities'] = complexity.mean().item()\n",
        "            thinking_info['steps'] = think_steps.float().mean().item()\n",
        "            thinking_states_list = [current_state.clone()] # Store initial state\n",
        "\n",
        "            # Execute thinking steps\n",
        "            for step in range(think_steps.max().item()):\n",
        "                # Only apply thinking step for batches that require this step\n",
        "                mask = think_steps > step\n",
        "                if mask.any():\n",
        "                    current_state[mask] = self.thinking.thinking_step(current_state[mask], self.time_mixing.wkv)\n",
        "                    thinking_states_list.append(current_state.clone())\n",
        "\n",
        "            thinking_info['states'] = thinking_states_list\n",
        "\n",
        "        # Standard RWKV-7 forward pass\n",
        "        # The WKV function needs to be able to handle the state argument\n",
        "        x = x + self.time_mixing(self.ln1(x), current_state)\n",
        "        x = x + self.channel_mixing(self.ln2(x))\n",
        "\n",
        "        return x, current_state, thinking_info\n",
        "\n",
        "class RWKV7TimeMixing(nn.Module):\n",
        "    \"\"\"RWKV-7 Time Mixing implementation\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Receptance, Key, Value, Gate projections\n",
        "        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.gate = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.output = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "\n",
        "        # Time decay and bonus parameters\n",
        "        # Shape should be [H, D]\n",
        "        self.time_decay = nn.Parameter(torch.zeros(config.n_head, config.head_dim))\n",
        "        self.time_bonus = nn.Parameter(torch.zeros(config.n_head, config.head_dim))\n",
        "\n",
        "        # Layer-specific initialization\n",
        "        ratio_0_to_1 = layer_idx / max(config.n_layer - 1, 1)\n",
        "        ratio_1_to_almost0 = 1.0 - (layer_idx / config.n_layer)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.orthogonal_(self.receptance.weight, gain=1)\n",
        "        nn.init.orthogonal_(self.key.weight, gain=0.1)\n",
        "        nn.init.orthogonal_(self.value.weight, gain=1)\n",
        "        nn.init.orthogonal_(self.gate.weight, gain=0.1)\n",
        "        nn.init.zeros_(self.output.weight)\n",
        "\n",
        "        # Initialize time decay\n",
        "        with torch.no_grad():\n",
        "            # RWKV-7 decay initialization\n",
        "            decay_values = -5 + 8 * ratio_0_to_1\n",
        "            self.time_decay.uniform_(decay_values - 0.5, decay_values + 0.5)\n",
        "\n",
        "    def wkv(self, state, k, v, decay):\n",
        "        \"\"\"WKV computation for RWKV-7\"\"\"\n",
        "        # This needs to be the actual efficient CUDA/optimized implementation\n",
        "        # The current implementation is a simplified placeholder and will be slow\n",
        "        # For training, you'll need the official RWKV CUDA kernel\n",
        "        # https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/cuda/wkv_cuda.py\n",
        "\n",
        "        # Simplified placeholder WKV (DO NOT USE FOR REAL TRAINING)\n",
        "        B, H, T, D = k.shape\n",
        "\n",
        "        # state shape should be [B, H, D, D] for full matrix state in RWKV-7\n",
        "        # Or [B, H, 2, D] for simplified state\n",
        "        # The current state shape in GazelleModel is None or handled by GhostState.\n",
        "        # Need to align state representation. Assuming state is None or [B, H, D, D] for now.\n",
        "\n",
        "        output = torch.zeros_like(v)\n",
        "        current_state = state if state is not None else torch.zeros(B, H, D, D, device=k.device, dtype=k.dtype)\n",
        "\n",
        "        for t in range(T):\n",
        "            # This is NOT the correct RWKV-7 state update\n",
        "            # This is a generic RNN-like update for the placeholder\n",
        "            current_state = current_state * decay.unsqueeze(-1) + k[:, :, t:t+1].transpose(-1, -2) @ v[:, :, t:t+1]\n",
        "            output[:, :, t] = (current_state @ k[:, :, t:t+1].transpose(-1, -2)).squeeze(-1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, state=None):\n",
        "        B, T, C = x.shape\n",
        "        H = self.config.n_head\n",
        "        D = self.config.head_dim\n",
        "\n",
        "        # Compute RKWG\n",
        "        r = self.receptance(x).view(B, T, H, D).transpose(1, 2)\n",
        "        k = self.key(x).view(B, T, H, D).transpose(1, 2)\n",
        "        v = self.value(x).view(B, T, H, D).transpose(1, 2)\n",
        "        g = torch.sigmoid(self.gate(x)).view(B, T, H, D).transpose(1, 2)\n",
        "\n",
        "        # Apply WKV\n",
        "        # Need to pass state to WKV\n",
        "        x = self.wkv(state, k, v, torch.exp(-torch.exp(self.time_decay)))\n",
        "\n",
        "        # Apply receptance and gate\n",
        "        x = x * torch.sigmoid(r) * g\n",
        "\n",
        "        # Combine heads and output\n",
        "        x = x.transpose(1, 2).reshape(B, T, C)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class RWKV7ChannelMixing(nn.Module):\n",
        "    \"\"\"RWKV-7 Channel Mixing (FFN) implementation\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # FFN components\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd * 4, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd * 4, config.n_embd, bias=False)\n",
        "        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "\n",
        "        # Initialize\n",
        "        nn.init.orthogonal_(self.key.weight, gain=1)\n",
        "        nn.init.zeros_(self.value.weight)\n",
        "        nn.init.zeros_(self.receptance.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # RWKV-7 style FFN with receptance gate\n",
        "        k = self.key(x)\n",
        "        k = torch.relu(k) ** 2  # Square ReLU activation\n",
        "        v = self.value(k)\n",
        "        r = torch.sigmoid(self.receptance(x))\n",
        "\n",
        "        return r * v\n",
        "\n",
        "class GazelleModel(nn.Module):\n",
        "    \"\"\"Complete Gazelle 0.5B Model\"\"\"\n",
        "\n",
        "    def __init__(self, config: GazelleConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Token embeddings\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.ln_emb = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        # Gazelle layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            GazelleRWKV7Layer(config, idx) for idx in range(config.n_layer)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.ln_head = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize embeddings\n",
        "        nn.init.uniform_(self.embedding.weight, -1e-4, 1e-4)\n",
        "        nn.init.orthogonal_(self.head.weight, gain=0.5 * math.sqrt(config.vocab_size / config.n_embd))\n",
        "\n",
        "        # State management - list of states, one per layer\n",
        "        self.states = [None] * self.config.n_layer\n",
        "\n",
        "        # Thinking info storage\n",
        "        self.thinking_info = {}\n",
        "\n",
        "        print(f\"✓ Gazelle Model initialized with {self.num_parameters():.2f}M parameters\")\n",
        "        if config.use_ghost:\n",
        "            print(f\"  - Ghost ratio: {config.ghost_ratio:.1%} reduction\")\n",
        "        if config.enable_thinking:\n",
        "            print(f\"  - Thinking enabled: max {config.max_think_steps} steps\")\n",
        "\n",
        "    def num_parameters(self):\n",
        "        \"\"\"Count trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad) / 1e6\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Token embeddings\n",
        "        x = self.embedding(idx)\n",
        "        x = self.ln_emb(x)\n",
        "\n",
        "        # Clear thinking info\n",
        "        self.thinking_info = {'complexities': [], 'steps': [], 'state_changes': []}\n",
        "\n",
        "        # Forward through layers\n",
        "        new_states = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, new_state, layer_thinking_info = layer(x, self.states[i])\n",
        "            new_states.append(new_state)\n",
        "\n",
        "            # Accumulate thinking info\n",
        "            if self.config.enable_thinking and layer.thinking is not None:\n",
        "                self.thinking_info['complexities'].append(layer_thinking_info.get('complexities', 0.0))\n",
        "                self.thinking_info['steps'].append(layer_thinking_info.get('steps', 0.0))\n",
        "                # State change info is calculated in ThinkingLossCalculator\n",
        "\n",
        "        self.states = new_states # Update states\n",
        "\n",
        "        # Output projection\n",
        "        x = self.ln_head(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        # Calculate loss if targets provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Reshape for cross_entropy: [N, C] and [N]\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_tokens=100, temperature=1.0, top_p=0.9):\n",
        "        \"\"\"Generate text with thinking steps tracking\"\"\"\n",
        "        self.eval()\n",
        "        generated = []\n",
        "        thinking_log = []\n",
        "        current_idx = idx.clone()\n",
        "\n",
        "        # Initialize states for generation\n",
        "        self.states = [None] * self.config.n_layer\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_tokens):\n",
        "                # Forward pass (process only the last token)\n",
        "                logits, _ = self(current_idx[:, -1].unsqueeze(0)) # Process one token at a time\n",
        "\n",
        "                logits = logits[:, -1, :] / temperature\n",
        "\n",
        "                # Top-p sampling\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "                cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
        "                mask = cumsum > top_p\n",
        "                mask[:, 0] = False  # Keep at least one token\n",
        "                sorted_probs[mask] = 0\n",
        "                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "                # Sample\n",
        "                next_token = torch.multinomial(sorted_probs, 1)\n",
        "                next_token = sorted_indices.gather(-1, next_token)\n",
        "\n",
        "                # Track thinking if enabled (aggregate across layers)\n",
        "                if self.config.enable_thinking and self.thinking_info:\n",
        "                    avg_complexity = sum(self.thinking_info.get('complexities', [])) / max(1, len(self.thinking_info.get('complexities', [])))\n",
        "                    avg_steps = sum(self.thinking_info.get('steps', [])) / max(1, len(self.thinking_info.get('steps', [])))\n",
        "\n",
        "                    thinking_log.append({\n",
        "                        'token_id': next_token.item(),\n",
        "                        'avg_complexity': avg_complexity,\n",
        "                        'avg_think_steps': avg_steps\n",
        "                    })\n",
        "                    # Clear thinking info after processing one token\n",
        "                    self.thinking_info = {'complexities': [], 'steps': [], 'state_changes': []}\n",
        "\n",
        "\n",
        "                # Append to sequence\n",
        "                current_idx = torch.cat([current_idx, next_token], dim=1)\n",
        "                generated.append(next_token.item())\n",
        "\n",
        "                # Stop on EOS token (assuming 0 is EOS, adjust if needed)\n",
        "                if next_token.item() == 0:\n",
        "                    break\n",
        "\n",
        "        return generated, thinking_log\n",
        "\n",
        "# Save the configuration\n",
        "config = GazelleConfig()\n",
        "# Ensure the directory exists before saving\n",
        "config_save_dir = '/content/gazelle_checkpoints/configs'\n",
        "os.makedirs(config_save_dir, exist_ok=True)\n",
        "torch.save(config, f'{config_save_dir}/base_config.pt')\n",
        "print(\"✓ Gazelle model architecture defined and saved!\")"
      ],
      "metadata": {
        "id": "dD8usSfZYQCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7: Training Utilities and Data Loading"
      ],
      "metadata": {
        "id": "R-0fNSINYWoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Utilities and Data Loading\n",
        "%%writefile gazelle_implementation/training_utils.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import os\n",
        "from contextlib import contextmanager\n",
        "\n",
        "class MemoryEfficientDataset(Dataset):\n",
        "    \"\"\"Memory-mapped dataset for Colab's limited RAM\"\"\"\n",
        "\n",
        "    def __init__(self, data_path, idx_path, ctx_len=512, epoch_length=None):\n",
        "        self.ctx_len = ctx_len\n",
        "\n",
        "        # Memory map the data files\n",
        "        self.data = np.memmap(data_path, dtype=np.uint16, mode='r')\n",
        "        self.idx = np.load(idx_path).astype(np.int64)\n",
        "\n",
        "        # Calculate actual data length\n",
        "        self.data_length = len(self.idx) - 1\n",
        "        self.epoch_length = epoch_length or self.data_length\n",
        "\n",
        "        print(f\"✓ Dataset initialized:\")\n",
        "        print(f\"  - Total tokens: {len(self.data):,}\")\n",
        "        print(f\"  - Total samples: {self.data_length:,}\")\n",
        "        print(f\"  - Context length: {ctx_len}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.epoch_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Use modulo for cycling through data\n",
        "        real_idx = idx % self.data_length\n",
        "\n",
        "        # Get start and end positions\n",
        "        start = self.idx[real_idx]\n",
        "        end = self.idx[real_idx + 1]\n",
        "\n",
        "        # Extract chunk\n",
        "        chunk = self.data[start:end]\n",
        "\n",
        "        # Pad or truncate to ctx_len\n",
        "        if len(chunk) >= self.ctx_len + 1:\n",
        "            # Random offset for variety\n",
        "            offset = np.random.randint(0, len(chunk) - self.ctx_len)\n",
        "            chunk = chunk[offset:offset + self.ctx_len + 1]\n",
        "        else:\n",
        "            # Pad if too short\n",
        "            chunk = np.pad(chunk, (0, self.ctx_len + 1 - len(chunk)), constant_values=0)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        x = torch.from_numpy(chunk[:-1].astype(np.int64))\n",
        "        y = torch.from_numpy(chunk[1:].astype(np.int64))\n",
        "\n",
        "        return x, y\n",
        "\n",
        "class ColabCheckpointer:\n",
        "    \"\"\"Handle checkpointing with Colab session management\"\"\"\n",
        "\n",
        "    def __init__(self, save_dir, max_runtime_hours=11.5):\n",
        "        self.save_dir = save_dir\n",
        "        self.start_time = time.time()\n",
        "        self.max_runtime = max_runtime_hours * 3600\n",
        "        self.checkpoint_counter = 0\n",
        "\n",
        "    def should_checkpoint(self, force=False):\n",
        "        \"\"\"Check if we should save a checkpoint\"\"\"\n",
        "        elapsed = time.time() - self.start_time\n",
        "\n",
        "        # Save if approaching Colab timeout\n",
        "        if elapsed > self.max_runtime - 600:  # 10 min buffer\n",
        "            return True, \"approaching_timeout\"\n",
        "\n",
        "        # Regular interval saves (every 30 min)\n",
        "        if elapsed > (self.checkpoint_counter + 1) * 1800:\n",
        "            return True, \"regular_interval\"\n",
        "\n",
        "        return force, \"forced\" if force else \"\"\n",
        "\n",
        "    def save(self, model, optimizer, epoch, step, metrics=None):\n",
        "        \"\"\"Save checkpoint with metadata\"\"\"\n",
        "        should_save, reason = self.should_checkpoint()\n",
        "\n",
        "        if should_save:\n",
        "            checkpoint = {\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'step': step,\n",
        "                'metrics': metrics or {},\n",
        "                'config': model.config,\n",
        "                'timestamp': time.time(),\n",
        "                'reason': reason\n",
        "            }\n",
        "\n",
        "            # Save with descriptive filename\n",
        "            filename = f\"gazelle_e{epoch}_s{step}_{reason}.pt\"\n",
        "            path = os.path.join(self.save_dir, filename)\n",
        "\n",
        "            torch.save(checkpoint, path)\n",
        "            self.checkpoint_counter += 1\n",
        "\n",
        "            print(f\"✓ Checkpoint saved: {filename}\")\n",
        "\n",
        "            # Also save a 'latest' symlink\n",
        "            latest_path = os.path.join(self.save_dir, \"latest.pt\")\n",
        "            if os.path.exists(latest_path):\n",
        "                os.unlink(latest_path)\n",
        "            os.symlink(path, latest_path)\n",
        "\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def load_latest(self, model, optimizer=None):\n",
        "        \"\"\"Load the most recent checkpoint\"\"\"\n",
        "        latest_path = os.path.join(self.save_dir, \"latest.pt\")\n",
        "\n",
        "        if os.path.exists(latest_path):\n",
        "            checkpoint = torch.load(latest_path)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "            if optimizer is not None:\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "            print(f\"✓ Loaded checkpoint from epoch {checkpoint['epoch']}, step {checkpoint['step']}\")\n",
        "            return checkpoint\n",
        "\n",
        "        return None\n",
        "\n",
        "class GradientAccumulator:\n",
        "    \"\"\"Handle gradient accumulation for larger effective batch sizes\"\"\"\n",
        "\n",
        "    def __init__(self, accumulation_steps=16):\n",
        "        self.accumulation_steps = accumulation_steps\n",
        "        self.step_count = 0\n",
        "\n",
        "    @contextmanager\n",
        "    def accumulate(self, model):\n",
        "        \"\"\"Context manager for gradient accumulation\"\"\"\n",
        "        self.step_count += 1\n",
        "\n",
        "        # Scale gradients by accumulation steps\n",
        "        if self.step_count < self.accumulation_steps:\n",
        "            # Don't sync gradients yet\n",
        "            with model.no_sync() if hasattr(model, 'no_sync') else contextlib.nullcontext():\n",
        "                yield self.accumulation_steps\n",
        "        else:\n",
        "            # Time to sync and step\n",
        "            yield self.accumulation_steps\n",
        "            self.step_count = 0\n",
        "\n",
        "class ThinkingLossCalculator:\n",
        "    \"\"\"Calculate auxiliary losses for thinking mechanism\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.state_change_weight = 0.1\n",
        "        self.convergence_weight = 0.05\n",
        "        self.efficiency_weight = 0.02\n",
        "\n",
        "    def calculate(self, thinking_states, complexities, actual_steps):\n",
        "        \"\"\"Calculate thinking-specific losses\"\"\"\n",
        "        losses = {}\n",
        "\n",
        "        if not thinking_states or len(thinking_states) < 2:\n",
        "            return losses\n",
        "\n",
        "        # State change loss - ensure thinking steps are meaningful\n",
        "        state_changes = []\n",
        "        for i in range(len(thinking_states) - 1):\n",
        "            change = torch.norm(thinking_states[i+1] - thinking_states[i], dim=-1)\n",
        "            state_changes.append(change)\n",
        "\n",
        "        state_change_loss = -torch.stack(state_changes).mean() * self.state_change_weight\n",
        "        losses['state_change'] = state_change_loss\n",
        "\n",
        "        # Convergence loss - encourage diminishing changes\n",
        "        if len(state_changes) > 1:\n",
        "            convergence_ratios = []\n",
        "            for i in range(len(state_changes) - 1):\n",
        "                ratio = state_changes[i+1] / (state_changes[i] + 1e-6)\n",
        "                convergence_ratios.append(ratio)\n",
        "\n",
        "            convergence_loss = torch.stack(convergence_ratios).mean() * self.convergence_weight\n",
        "            losses['convergence'] = convergence_loss\n",
        "\n",
        "        # Efficiency loss - penalize unnecessary thinking\n",
        "        efficiency_loss = (actual_steps.float() - complexities).pow(2).mean() * self.efficiency_weight\n",
        "        losses['efficiency'] = efficiency_loss\n",
        "\n",
        "        return losses\n",
        "\n",
        "class TrainingMetrics:\n",
        "    \"\"\"Track and log training metrics\"\"\"\n",
        "\n",
        "    def __init__(self, log_interval=100):\n",
        "        self.log_interval = log_interval\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.thinking_stats = {\n",
        "            'avg_steps': [],\n",
        "            'complexity': [],\n",
        "            'state_changes': []\n",
        "        }\n",
        "        self.step = 0\n",
        "\n",
        "    def update(self, loss, lr, thinking_info=None):\n",
        "        \"\"\"Update metrics\"\"\"\n",
        "        self.losses.append(loss)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        if thinking_info:\n",
        "            for key, value in thinking_info.items():\n",
        "                if key in self.thinking_stats:\n",
        "                    self.thinking_stats[key].append(value)\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "        # Log at intervals\n",
        "        if self.step % self.log_interval == 0:\n",
        "            self.log()\n",
        "\n",
        "    def log(self):\n",
        "        \"\"\"Print current metrics\"\"\"\n",
        "        avg_loss = np.mean(self.losses[-self.log_interval:])\n",
        "        current_lr = self.lrs[-1]\n",
        "\n",
        "        print(f\"\\nStep {self.step}:\")\n",
        "        print(f\"  Loss: {avg_loss:.4f}\")\n",
        "        print(f\"  LR: {current_lr:.2e}\")\n",
        "\n",
        "        if any(len(v) > 0 for v in self.thinking_stats.values()):\n",
        "            print(\"  Thinking stats:\")\n",
        "            for key, values in self.thinking_stats.items():\n",
        "                if values:\n",
        "                    print(f\"    {key}: {np.mean(values[-self.log_interval:]):.3f}\")\n",
        "\n",
        "# Memory monitoring utilities\n",
        "def log_gpu_memory():\n",
        "    \"\"\"Log current GPU memory usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
        "\n",
        "def optimize_memory():\n",
        "    \"\"\"Clear GPU cache and optimize memory\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "print(\"✓ Training utilities defined!\")"
      ],
      "metadata": {
        "id": "TZOJwG1UYYaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8: Main Training Script"
      ],
      "metadata": {
        "id": "D-eAM4X3YbkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Main Training Script for Gazelle 0.5B\n",
        "%%writefile train_gazelle.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Add implementation to path\n",
        "sys.path.append('gazelle_implementation')\n",
        "\n",
        "from gazelle_model import GazelleModel, GazelleConfig\n",
        "from training_utils import (\n",
        "    MemoryEfficientDataset, ColabCheckpointer, GradientAccumulator,\n",
        "    ThinkingLossCalculator, TrainingMetrics, log_gpu_memory, optimize_memory\n",
        ")\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Cosine learning rate schedule with warmup\"\"\"\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "class GazelleTrainer:\n",
        "    \"\"\"Main trainer class for Gazelle model\"\"\"\n",
        "\n",
        "    def __init__(self, config, checkpoint_dir):\n",
        "        self.config = config\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = GazelleModel(config).to(self.device)\n",
        "\n",
        "        # Training components\n",
        "        self.optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=3e-4,\n",
        "            betas=(0.9, 0.99),\n",
        "            weight_decay=0.1,\n",
        "            eps=1e-8\n",
        "        )\n",
        "\n",
        "        # Mixed precision training\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "        # Gradient accumulation\n",
        "        self.accumulator = GradientAccumulator(accumulation_steps=16)\n",
        "\n",
        "        # Checkpointing\n",
        "        self.checkpointer = ColabCheckpointer(checkpoint_dir)\n",
        "\n",
        "        # Metrics\n",
        "        self.metrics = TrainingMetrics()\n",
        "\n",
        "        # Thinking loss (if enabled)\n",
        "        if config.enable_thinking:\n",
        "            self.thinking_loss_calc = ThinkingLossCalculator(config)\n",
        "\n",
        "        # Load dataset\n",
        "        self.train_dataset = MemoryEfficientDataset(\n",
        "            'data/minipile.bin',\n",
        "            'data/minipile.idx',\n",
        "            ctx_len=config.ctx_len\n",
        "        )\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=8,  # Small batch for Colab\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Calculate training steps\n",
        "        self.steps_per_epoch = len(self.train_loader) // self.accumulator.accumulation_steps\n",
        "        self.total_steps = self.steps_per_epoch * 10  # 10 epochs initially\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = get_cosine_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=100,\n",
        "            num_training_steps=self.total_steps\n",
        "        )\n",
        "\n",
        "        # Try to load checkpoint\n",
        "        checkpoint = self.checkpointer.load_latest(self.model, self.optimizer)\n",
        "        if checkpoint:\n",
        "            self.start_epoch = checkpoint['epoch']\n",
        "            self.global_step = checkpoint['step']\n",
        "        else:\n",
        "            self.start_epoch = 0\n",
        "            self.global_step = 0\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(self.train_loader):\n",
        "            # Move to device\n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with autocast():\n",
        "                with self.accumulator.accumulate(self.model) as acc_steps:\n",
        "                    logits, loss = self.model(x, y)\n",
        "\n",
        "                    # Scale loss by accumulation steps\n",
        "                    loss = loss / acc_steps\n",
        "\n",
        "                    # Add thinking losses if enabled\n",
        "                    if self.config.enable_thinking and hasattr(self.model, 'thinking_info'):\n",
        "                        thinking_losses = self.thinking_loss_calc.calculate(\n",
        "                            self.model.thinking_info.get('states', []),\n",
        "                            self.model.thinking_info.get('complexities', []),\n",
        "                            self.model.thinking_info.get('steps', [])\n",
        "                        )\n",
        "\n",
        "                        for t_loss in thinking_losses.values():\n",
        "                            loss = loss + t_loss / acc_steps\n",
        "\n",
        "            # Backward pass\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            # Optimizer step (only when accumulator says so)\n",
        "            if self.accumulator.step_count == 0:\n",
        "                # Gradient clipping\n",
        "                self.scaler.unscale_(self.optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                # Optimizer step\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Scheduler step\n",
        "                self.scheduler.step()\n",
        "\n",
        "                # Update metrics\n",
        "                self.metrics.update(\n",
        "                    loss.item() * acc_steps,\n",
        "                    self.scheduler.get_last_lr()[0]\n",
        "                )\n",
        "\n",
        "                self.global_step += 1\n",
        "\n",
        "            # Checkpointing\n",
        "            if self.global_step > 0 and self.global_step % 100 == 0:\n",
        "                self.checkpointer.save(\n",
        "                    self.model, self.optimizer, epoch, self.global_step,\n",
        "                    {'loss': loss.item() * acc_steps}\n",
        "                )\n",
        "\n",
        "            # Memory management\n",
        "            if batch_idx % 50 == 0:\n",
        "                optimize_memory()\n",
        "                log_gpu_memory()\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(f\"\\nEpoch {epoch} completed in {epoch_time:.1f}s\")\n",
        "        print(f\"Average time per step: {epoch_time/self.steps_per_epoch:.2f}s\")\n",
        "\n",
        "    def train(self, num_epochs=None):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        num_epochs = num_epochs or (10 - self.start_epoch)\n",
        "\n",
        "        print(f\"\\nStarting training:\")\n",
        "        print(f\"  Model parameters: {self.model.num_parameters():.2f}M\")\n",
        "        print(f\"  Batch size: 8 x {self.accumulator.accumulation_steps} = {8 * self.accumulator.accumulation_steps}\")\n",
        "        print(f\"  Total steps: {self.total_steps}\")\n",
        "        print(f\"  Starting from epoch: {self.start_epoch}\")\n",
        "\n",
        "        try:\n",
        "            for epoch in range(self.start_epoch, self.start_epoch + num_epochs):\n",
        "                print(f\"\\n{'='*50}\")\n",
        "                print(f\"Epoch {epoch + 1}/{self.start_epoch + num_epochs}\")\n",
        "                print(f\"{'='*50}\")\n",
        "\n",
        "                self.train_epoch(epoch)\n",
        "\n",
        "                # Save end-of-epoch checkpoint\n",
        "                self.checkpointer.save(\n",
        "                    self.model, self.optimizer, epoch + 1, self.global_step,\n",
        "                    {'epoch_complete': True}\n",
        "                )\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nTraining interrupted! Saving checkpoint...\")\n",
        "            self.checkpointer.save(\n",
        "                self.model, self.optimizer, epoch, self.global_step,\n",
        "                {'interrupted': True}\n",
        "            )\n",
        "\n",
        "        print(\"\\n✓ Training completed!\")\n",
        "        return self.model\n",
        "\n",
        "# Configuration for phased training\n",
        "def get_phase_config(phase):\n",
        "    \"\"\"Get configuration for different training phases\"\"\"\n",
        "    base_config = GazelleConfig()\n",
        "\n",
        "    if phase == 1:\n",
        "        # Phase 1: Baseline RWKV-7\n",
        "        base_config.use_ghost = False\n",
        "        base_config.enable_thinking = False\n",
        "        print(\"Phase 1: Baseline RWKV-7 training\")\n",
        "\n",
        "    elif phase == 2:\n",
        "        # Phase 2: Add GhostRNN\n",
        "        base_config.use_ghost = True\n",
        "        base_config.enable_thinking = False\n",
        "        print(\"Phase 2: GhostRNN integration\")\n",
        "\n",
        "    elif phase == 3:\n",
        "        # Phase 3: Add Thinking\n",
        "        base_config.use_ghost = True\n",
        "        base_config.enable_thinking = True\n",
        "        base_config.max_think_steps = 3  # Start small\n",
        "        print(\"Phase 3: Thinking mechanism\")\n",
        "\n",
        "    elif phase == 4:\n",
        "        # Phase 4: Full model\n",
        "        base_config.use_ghost = True\n",
        "        base_config.enable_thinking = True\n",
        "        base_config.max_think_steps = 5\n",
        "        print(\"Phase 4: Full Gazelle model\")\n",
        "\n",
        "    return base_config\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--phase', type=int, default=1, help='Training phase (1-4)')\n",
        "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='/content/gazelle_checkpoints/models')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Get config for current phase\n",
        "    config = get_phase_config(args.phase)\n",
        "\n",
        "    # Save config\n",
        "    config_path = f\"{args.checkpoint_dir}/phase{args.phase}_config.json\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config.__dict__, f, indent=2)\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = GazelleTrainer(config, args.checkpoint_dir)\n",
        "\n",
        "    # Train model\n",
        "    model = trainer.train(args.epochs)\n",
        "\n",
        "    # Save final model\n",
        "    final_path = f\"{args.checkpoint_dir}/gazelle_phase{args.phase}_final.pt\"\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f\"✓ Final model saved to {final_path}\")\n",
        "\n",
        "print(\"✓ Training script created!\")"
      ],
      "metadata": {
        "id": "bokpC89HYdY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9: Run Phase 1 Training (Baseline RWKV-7)"
      ],
      "metadata": {
        "id": "zg1sN0vDYm7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what's in the data directory\n",
        "!ls -la /content/RWKV-LM/data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5sP52cMsa7w",
        "outputId": "b35d74dc-5d8c-4af7-c6ba-047cfb4728d2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x  2 root root 4096 Jul  2 05:54 .\n",
            "drwxr-xr-x 17 root root 4096 Jul  2 05:52 ..\n",
            "-rw-r--r--  1 root root    0 Jul  2 05:44 20B_tokenizer.json\n",
            "lrwxrwxrwx  1 root root   19 Jul  2 05:54 minipile.bin -> dolphin_distill.bin\n",
            "lrwxrwxrwx  1 root root   23 Jul  2 05:54 minipile.idx -> dolphin_distill.idx.npy\n",
            "lrwxrwxrwx  1 root root   23 Jul  2 05:54 minipile_val.bin -> dolphin_distill_val.bin\n",
            "lrwxrwxrwx  1 root root   27 Jul  2 05:54 minipile_val.idx -> dolphin_distill_val.idx.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the expected data directory if it doesn't exist\n",
        "!mkdir -p /content/RWKV-LM/data\n",
        "\n",
        "# Copy or move the files to where the script expects them\n",
        "# Adjust the source path based on what the find command shows\n",
        "!cp /content/data/dolphin_distill* /content/RWKV-LM/data/\n",
        "!ls -la /content/RWKV-LM/data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfwzJDTVsdNe",
        "outputId": "5ae160f4-67d1-453f-b166-61625e63b34b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/data/dolphin_distill*': No such file or directory\n",
            "total 8\n",
            "drwxr-xr-x  2 root root 4096 Jul  2 05:54 .\n",
            "drwxr-xr-x 17 root root 4096 Jul  2 05:52 ..\n",
            "-rw-r--r--  1 root root    0 Jul  2 05:44 20B_tokenizer.json\n",
            "lrwxrwxrwx  1 root root   19 Jul  2 05:54 minipile.bin -> dolphin_distill.bin\n",
            "lrwxrwxrwx  1 root root   23 Jul  2 05:54 minipile.idx -> dolphin_distill.idx.npy\n",
            "lrwxrwxrwx  1 root root   23 Jul  2 05:54 minipile_val.bin -> dolphin_distill_val.bin\n",
            "lrwxrwxrwx  1 root root   27 Jul  2 05:54 minipile_val.idx -> dolphin_distill_val.idx.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for the data files\n",
        "!find /content -name \"dolphin_distill.bin\" -type f 2>/dev/null\n",
        "!find /content -name \"*.bin\" -type f 2>/dev/null | grep -E \"(dolphin|minipile)\"\n",
        "\n",
        "# Check current working directory\n",
        "!pwd\n",
        "\n",
        "# List what's in various data directories\n",
        "!ls -la /content/data/ 2>/dev/null || echo \"No /content/data/\"\n",
        "!ls -la /content/RWKV-LM/data/ 2>/dev/null || echo \"No /content/RWKV-LM/data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr2yFEqRsqjD",
        "outputId": "5a8f674c-0c90-40ae-c31d-1877c1daaaa8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RWKV-LM\n",
            "No /content/data/\n",
            "total 8\n",
            "drwxr-xr-x  2 root root 4096 Jul  2 05:54 .\n",
            "drwxr-xr-x 17 root root 4096 Jul  2 05:52 ..\n",
            "-rw-r--r--  1 root root    0 Jul  2 05:44 20B_tokenizer.json\n",
            "lrwxrwxrwx  1 root root   19 Jul  2 05:54 minipile.bin -> dolphin_distill.bin\n",
            "lrwxrwxrwx  1 root root   23 Jul  2 05:54 minipile.idx -> dolphin_distill.idx.npy\n",
            "lrwxrwxrwx  1 root root   23 Jul  2 05:54 minipile_val.bin -> dolphin_distill_val.bin\n",
            "lrwxrwxrwx  1 root root   27 Jul  2 05:54 minipile_val.idx -> dolphin_distill_val.idx.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/RWKV-LM/data\n",
        "\n",
        "# Create symbolic links\n",
        "!ln -sf dolphin_distill.bin minipile.bin\n",
        "!ln -sf dolphin_distill.idx.npy minipile.idx\n",
        "!ln -sf dolphin_distill_val.bin minipile_val.bin\n",
        "!ln -sf dolphin_distill_val.idx.npy minipile_val.idx\n",
        "\n",
        "# Verify links\n",
        "!ls -la *.bin *.idx*\n",
        "%cd /content/RWKV-LM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I0QnXaHs3Gz",
        "outputId": "831c1a2a-9795-4fa8-85cd-f3066e4686f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RWKV-LM/data\n",
            "lrwxrwxrwx 1 root root 19 Jul  2 05:58 minipile.bin -> dolphin_distill.bin\n",
            "lrwxrwxrwx 1 root root 23 Jul  2 05:58 minipile.idx -> dolphin_distill.idx.npy\n",
            "lrwxrwxrwx 1 root root 23 Jul  2 05:58 minipile_val.bin -> dolphin_distill_val.bin\n",
            "lrwxrwxrwx 1 root root 27 Jul  2 05:58 minipile_val.idx -> dolphin_distill_val.idx.npy\n",
            "/content/RWKV-LM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check where train_gazelle.py is looking for data\n",
        "!grep -n \"data/\" /content/RWKV-LM/train_gazelle.py | head -10\n",
        "\n",
        "# Replace with absolute paths (adjust based on where your files actually are)\n",
        "!sed -i \"s|'data/dolphin_distill.bin'|'/content/data/dolphin_distill.bin'|g\" /content/RWKV-LM/train_gazelle.py\n",
        "!sed -i \"s|'data/dolphin_distill.idx.npy'|'/content/data/dolphin_distill.idx.npy'|g\" /content/RWKV-LM/train_gazelle.py\n",
        "!sed -i \"s|'data/dolphin_distill_val.bin'|'/content/data/dolphin_distill_val.bin'|g\" /content/RWKV-LM/train_gazelle.py\n",
        "!sed -i \"s|'data/dolphin_distill_val.idx.npy'|'/content/data/dolphin_distill_val.idx.npy'|g\" /content/RWKV-LM/train_gazelle.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EKz8g2YtoWB",
        "outputId": "e77598ce-6aa9-4f33-b93b-3e7378385f9a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68:            'data/dolphin_distill.bin',\n",
            "69:            'data/dolphin_distill.idx.npy',\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retry training\n",
        "!python train_gazelle.py --phase 1 --epochs 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF-O5iwCtBm8",
        "outputId": "e21318b3-e394-4df0-fab5-57ed97da5d4f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Training utilities defined!\n",
            "Phase 1: Baseline RWKV-7 training\n",
            "✓ Gazelle Model initialized with 597.81M parameters\n",
            "/content/RWKV-LM/train_gazelle.py:51: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler('cuda')\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/RWKV-LM/train_gazelle.py\", line 256, in <module>\n",
            "    trainer = GazelleTrainer(config, args.checkpoint_dir)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/RWKV-LM/train_gazelle.py\", line 67, in __init__\n",
            "    self.train_dataset = MemoryEfficientDataset(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/RWKV-LM/gazelle_implementation/training_utils.py\", line 17, in __init__\n",
            "    self.data = np.memmap(data_path, dtype=np.uint16, mode='r')\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numpy/_core/memmap.py\", line 233, in __new__\n",
            "    f_ctx = open(\n",
            "            ^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/data/dolphin_distill.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Phase 1: Train Baseline RWKV-7\n",
        "!python train_gazelle.py --phase 1 --epochs 5\n",
        "\n",
        "# Monitor training progress\n",
        "print(\"\\n✓ Phase 1 training initiated!\")\n",
        "print(\"Monitor the output above for training progress.\")\n",
        "print(\"Checkpoints will be saved automatically.\")"
      ],
      "metadata": {
        "id": "YbSKxfThYoWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10: Run Phase 2 Training (Add GhostRNN)"
      ],
      "metadata": {
        "id": "iApk8w0rYrEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Phase 2: Integrate GhostRNN\n",
        "# First, load the Phase 1 checkpoint\n",
        "import torch\n",
        "import sys\n",
        "sys.path.append('gazelle_implementation')\n",
        "from gazelle_model import GazelleConfig, GazelleModel\n",
        "\n",
        "# Load Phase 1 model\n",
        "phase1_checkpoint = torch.load('/content/gazelle_checkpoints/models/gazelle_phase1_final.pt')\n",
        "\n",
        "# Create Phase 2 config\n",
        "phase2_config = GazelleConfig()\n",
        "phase2_config.use_ghost = True\n",
        "phase2_config.enable_thinking = False\n",
        "\n",
        "# Initialize Phase 2 model\n",
        "phase2_model = GazelleModel(phase2_config)\n",
        "\n",
        "# Transfer weights where possible\n",
        "phase2_model.load_state_dict(phase1_checkpoint, strict=False)\n",
        "\n",
        "# Save as starting point for Phase 2\n",
        "torch.save({\n",
        "    'model_state_dict': phase2_model.state_dict(),\n",
        "    'optimizer_state_dict': None,\n",
        "    'epoch': 0,\n",
        "    'step': 0,\n",
        "    'config': phase2_config\n",
        "}, '/content/gazelle_checkpoints/models/latest.pt')\n",
        "\n",
        "print(\"✓ Phase 1 weights transferred to Phase 2 model\")\n",
        "\n",
        "# Run Phase 2 training\n",
        "!python train_gazelle.py --phase 2 --epochs 5"
      ],
      "metadata": {
        "id": "oc6SJFqKYqvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 11: Run Phase 3 Training (Add Thinking)"
      ],
      "metadata": {
        "id": "drP4ZrWDYuax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Phase 3: Add Thinking Mechanism\n",
        "# Similar transfer process for Phase 3\n",
        "!python train_gazelle.py --phase 3 --epochs 10\n",
        "\n",
        "print(\"\\n✓ Phase 3 training with thinking mechanism initiated!\")"
      ],
      "metadata": {
        "id": "Zt2PrORJYykY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 12: Interactive Demo"
      ],
      "metadata": {
        "id": "WmCCWcEkY2uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interactive Gazelle Demo\n",
        "import gradio as gr\n",
        "import torch\n",
        "import sys\n",
        "sys.path.append('gazelle_implementation')\n",
        "from gazelle_model import GazelleModel, GazelleConfig\n",
        "\n",
        "# Load model for demo\n",
        "model_path = '/content/gazelle_checkpoints/models/latest.pt'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(model_path)\n",
        "config = checkpoint.get('config', GazelleConfig())\n",
        "model = GazelleModel(config).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "def generate_text(prompt, max_length=100, temperature=0.8):\n",
        "    \"\"\"Generate text from prompt\"\"\"\n",
        "    # Simplified - you'd use proper tokenization\n",
        "    input_ids = torch.randint(0, 1000, (1, 10)).to(device)\n",
        "\n",
        "    generated, thinking_log = model.generate(\n",
        "        input_ids,\n",
        "        max_tokens=max_length,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    # Format output\n",
        "    output = f\"Generated text: [Model output would appear here]\\n\\n\"\n",
        "\n",
        "    if config.enable_thinking:\n",
        "        output += \"Thinking Analysis:\\n\"\n",
        "        avg_steps = np.mean([log['think_steps'] for log in thinking_log])\n",
        "        output += f\"Average thinking steps: {avg_steps:.2f}\\n\"\n",
        "\n",
        "    return output\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Input Prompt\", lines=3),\n",
        "        gr.Slider(50, 500, value=100, label=\"Max Length\"),\n",
        "        gr.Slider(0.1, 2.0, value=0.8, label=\"Temperature\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Output\", lines=10),\n",
        "    title=\"Gazelle 0.5B Demo\",\n",
        "    description=\"Test the Gazelle model with adaptive thinking\"\n",
        ")\n",
        "\n",
        "# Launch demo\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "LdnMhLBcY4T_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}