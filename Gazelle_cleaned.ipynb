{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01415be",
   "metadata": {},
   "source": [
    "# Gazelle 0.5B Training\n",
    "\n",
    "This notebook provides a cleaned setup for training and running the Gazelle 0.5B model. It consolidates the original steps into a simpler workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c16dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_name(0)\n",
    "    print(f'GPU: {gpu}')\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "else:\n",
    "    raise RuntimeError('GPU required')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be040298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading Dolphin Distill datasetâ€¦')\n",
    "dataset = load_dataset('cognitivecomputations/dolphin-distill', split='train')\n",
    "print('Dataset size:', len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51853571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GazelleConfig:\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 1536\n",
    "    n_head: int = 24\n",
    "    vocab_size: int = 65536\n",
    "    ctx_len: int = 512\n",
    "\n",
    "class GazelleModel(nn.Module):\n",
    "    def __init__(self, config: GazelleConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(config.n_embd, config.n_head)\n",
    "            for _ in range(config.n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1916b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = GazelleConfig()\n",
    "model = GazelleModel(config).cuda()\n",
    "\n",
    "# Simple tokenizer for demo\n",
    "tokenizer = AutoTokenizer.from_pretrained('BlinkDL/rwkv-4-world', use_fast=False)\n",
    "\n",
    "def encode(example):\n",
    "    ids = tokenizer(example['text'], truncation=True, padding='max_length',\n",
    "                    max_length=config.ctx_len, return_tensors='pt')\n",
    "    return {'input_ids': ids.input_ids[0]}\n",
    "\n",
    "dataset_enc = dataset.map(encode)\n",
    "loader = DataLoader(dataset_enc, batch_size=1)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for i, batch in enumerate(loader):\n",
    "    if i > 10:  # tiny demo\n",
    "        break\n",
    "    input_ids = batch['input_ids'].cuda()\n",
    "    logits = model(input_ids)\n",
    "    loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                                        input_ids.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i % 5 == 0:\n",
    "        print(f'step {i} loss {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce015fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = model(ids)\n",
    "            next_id = torch.argmax(logits[:, -1], dim=-1, keepdim=True)\n",
    "            ids = torch.cat([ids, next_id], dim=-1)\n",
    "            if next_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    return tokenizer.decode(ids[0])\n",
    "\n",
    "print(generate('Hello world'))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
